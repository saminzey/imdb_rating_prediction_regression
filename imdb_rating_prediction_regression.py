# -*- coding: utf-8 -*-
"""imdb_rating_prediction_regression

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uDc-08hbGUVrQOilXNMDMPV6KlTfYoU-
"""

from sklearn.linear_model import LinearRegression # I will be using multiple linear regression as one method to evaluate the dataset and make predictions.
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
from sklearn import metrics
import random
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
import numpy as np
from seaborn import load_dataset, pairplot
from sklearn.ensemble import RandomForestRegressor # My second method of evaluation is random forest trees.
from sklearn.tree import export_graphviz
import pydot
import warnings
warnings.filterwarnings('ignore')

#def main():

basic_info = pd.read_csv('basics.tsv', sep = '\t')
rating_info = pd.read_csv('ratings.tsv', sep = '\t')

print(basic_info.shape)
print(rating_info.shape)

master_df = pd.merge(rating_info, basic_info, on = ["tconst"]) # We must join the basics and ratings tables on the unique movie ID ('tconst').
print(master_df.shape)

#if __name__ == '__main__':
#	main()

# Let's take a look at some of our data.

print(master_df.head)

# endYear seems to be exclusively newline values. We will remove that column.
working_df = master_df.copy()
working_df = working_df.drop("endYear", axis = 1)

# Other columns, mainly startYear, runtimeMinutes, and genres seem to have some newline values. Let's remove the rows that have these values.
remove_newlines = working_df[working_df["startYear"] == '\\N'].index
working_df.drop(remove_newlines, inplace = True)

remove_newlines = working_df[working_df["runtimeMinutes"] == '\\N'].index
working_df.drop(remove_newlines, inplace = True)

remove_newlines = working_df[working_df["genres"] == '\\N'].index
working_df.drop(remove_newlines, inplace = True)

remove_newlines = working_df[working_df["numVotes"] == '\\N'].index
working_df.drop(remove_newlines, inplace = True)

print(working_df.shape)

# Now we must convert some of the columns to numerical values. The first column to change is titleType.
print(working_df.nunique()) # There are 10 unique titleTypes.

title_types = set(working_df["titleType"])
print(title_types) # {'movie', 'tvMovie', 'tvSpecial', 'videoGame', 'tvShort', 'tvEpisode', 'short', 'tvSeries', 'video', 'tvMiniSeries'}

title_dictionary = {'movie' : 0, 'tvMovie' : 1, 'tvSpecial' : 2, 'videoGame' : 3, 'tvShort' : 4, 'tvEpisode' : 5, 
                    'short' : 6, 'tvSeries' : 7, 'video' : 8, 'tvMiniSeries' : 9}
working_df['titleType'] = working_df['titleType'].apply(lambda x: title_dictionary[x])

title_types = set(working_df["titleType"])
print(title_types)

# isAdult is supposed to be a boolean field yet it has 4 different values. Let's see what is happening here.

isAdult_types = set(working_df["isAdult"])
print(isAdult_types) # {0, 1, '0', '1'}

isAdult_dictionary = {0 : 0, '0': 0, 1 : 1, '1' : 1}
working_df['isAdult'] = working_df['isAdult'].apply(lambda x: isAdult_dictionary[x])

isAdult_types = set(working_df["isAdult"])
print(isAdult_types)

# The genres feature has 1,949 unique values. It is very impractical to go through all 1,949 different values and create a dictionary.
# However, we can still use the feature. Let us determine how many genres a work has and change the field to represent that.
# We can do this by counting the number of commas in each entry. The max number of genres is 3.

print(working_df.shape)
genre_numbers = []
for row in working_df.itertuples():
  genre_list = row[-1].split(",")

  if len(genre_list) == 1:
    genre_numbers.append(1)

  elif len(genre_list) == 2:
    genre_numbers.append(2)

  else:
    genre_numbers.append(3)

working_df = working_df.drop("genres", axis = 1)
working_df['genres'] = genre_numbers
genre_types = set(working_df["genres"])
print(genre_types)
print(working_df.shape)
print(working_df.head)

# There are two more fields we can either modify or remove: primaryTitle and originalTitle.
# If we knew for sure how many languages are represented in the dataset it could be interesting to use the bigram model
# and change the originalTitle column to represent the language of the title of the film. That, however, would take weeks to develop ourselves.
# We can create a new column which represents the length of the title in characters and see if it is a meaningful feature.
# As the primaryTitle and originalTitle are often the same, we will just take the length of the primaryTitle.

length_of_titles = []

for row in working_df.itertuples():
  length_of_titles.append(len(row[-6]))


working_df = working_df.drop("primaryTitle", axis = 1)
working_df = working_df.drop("originalTitle", axis = 1)

working_df["length_title"] = length_of_titles
print(working_df.shape)
print(working_df.head)

# Let's perform some exploratory data analysis to remove outliers and see what the features look like.
# Let's start with some histograms of the features.
import scipy.stats as stats
print(working_df.describe())

plt.hist(working_df["averageRating"])
plt.title("Average Rating Histogram")
plt.xlabel("Rating")
plt.ylabel("Count")
plt.show()

# There is left skew in the histogram. The majority of works are rated between 6.0 and 8.0 it seems.
# Some works have a rating as low as a 1.0. Let's calculate some z-scores and remove some extreme outliers.
zscores = stats.zscore(working_df["averageRating"])
print(zscores)
# The vast majority of values are within three standard deviations of the mean. Let's remove any work with a value over three SD's away.

working_df["zscores"] = zscores

rating_remove = working_df[working_df["zscores"] < -3].index # We do not account for cases where the zscore > 3 SD's away as that would be a rating of over 10.0.
working_df.drop(rating_remove, inplace = True)
working_df = working_df.drop("zscores", axis = 1)
print(working_df.describe())

plt.hist(working_df["averageRating"])
plt.title("Average Rating Histogram")
plt.xlabel("Rating")
plt.ylabel("Count")
plt.show() # There is still left skew but this looks better.

# Now let's check the numVotes column.
pd.options.display.float_format = '{:20,.2f}'.format # Disabling scientific notation.
print(working_df.describe())
# The max and min values are *very* far apart (the SD is 20,767.50!). The max is so high that pandas was using scientific notation.
# I disabled this but matplotlib is also having trouble graphing because of this.
# We are going to have to remove some of the extreme outliers before graphing. Let's use z-scores again.
# The 75% quartile is 160 votes. So the SD is very likely skewed by the outliers. Let's remove values that are > 1 SD away.

zscores = stats.zscore(working_df["numVotes"])
print(zscores)

working_df["zscores"] = zscores

votes_remove = working_df[working_df["zscores"] > 1].index
working_df.drop(votes_remove, inplace = True)
working_df = working_df.drop("zscores", axis = 1)


print(working_df["numVotes"])
plt.hist(working_df["numVotes"])
plt.title("Number of Votes Histogram")
plt.xlabel("Number of Votes")
plt.ylabel("Count")
plt.show() # There is extreme left skew. The SD is now 1,470.18. So few of the rows have a numVotes > 10,000 and they are further skewing the graph. Let's remove them.
print(working_df.describe())


votes_remove = working_df[working_df["numVotes"] > 10000].index
working_df.drop(votes_remove, inplace = True)

print(working_df["numVotes"])
plt.hist(working_df["numVotes"])
plt.title("Number of Votes Histogram")
plt.xlabel("Number of Votes")
plt.ylabel("Count")
plt.show()

print(working_df.describe()) # The SD is now 887.24. If problems persist we will further remove values.

# Let's examine the titleType column. As a reminder, the significance of the numbers are:
# {'movie' : 0, 'tvMovie' : 1, 'tvSpecial' : 2, 'videoGame' : 3, 'tvShort' : 4, 'tvEpisode' : 5, 
#                    'short' : 6, 'tvSeries' : 7, 'video' : 8, 'tvMiniSeries' : 9}

#print(working_df["titleType"])
plt.hist(working_df["titleType"])
plt.title("Title Type Histogram")
plt.xlabel("Title Type")
plt.ylabel("Count")
plt.show() # It looks like there are very few works of type 3 and 4. Let's see the count of each.

print(working_df["titleType"].value_counts()) # There are only 83 of type 3 and 1,987 of type 4. Let's remove these and update the values. All other types have > 6,400 entries.

remove_type_three = working_df[working_df["titleType"] == 3].index
working_df.drop(remove_type_three, inplace = True)

remove_type_four = working_df[working_df["titleType"] == 4].index
working_df.drop(remove_type_four, inplace = True)

working_df["titleType"].replace(5, 3, inplace = True)
working_df["titleType"].replace(6, 4, inplace = True)
working_df["titleType"].replace(7, 5, inplace = True)
working_df["titleType"].replace(8, 6, inplace = True)
working_df["titleType"].replace(9, 7, inplace = True)
print(working_df["titleType"].value_counts())

plt.hist(working_df["titleType"])
plt.title("Title Type Histogram")
plt.xlabel("Title Type")
plt.ylabel("Count")
plt.show()

print(working_df.describe())

# The isAdult column refers to whether or not a work is an "adult title" (which is a bit ambiguous). 0 for no, 1 for yes.

print(working_df["isAdult"].value_counts())

print("Percentage of titles that are 'adult titles': ", str(100 * (14846 / (14846 + 886158))) + "%")

# There is extreme imbalance here. We will perform tests when the model is fit to see if this feature should be kept.

# Let's see the count for the genres column which we created earlier.

print(working_df["genres"].value_counts())
x_values = [1,2,3]
y_values = [302893, 275352, 322759]
plt.bar(x_values, y_values)
plt.title("Number of Genres For Each Work")
plt.xlabel("Number of Genres")
plt.ylabel("Count")
plt.xticks(np.arange(min(x_values), max(x_values)+1, 1.0))
plt.show() # This looks pretty normal and evenly distributed.

# The final feature to examine is length_title. This is a fairly straightforward reduction.
# Z-scores will be used again to eliminate outliers.

plt.hist(working_df["length_title"])
plt.title("Title Length Histogram")
plt.xlabel("Title Length")
plt.ylabel("Count")
plt.show() # Extreme right skew.

zscores = stats.zscore(working_df["length_title"])
print(zscores)

working_df["zscores"] = zscores

length_remove = working_df[working_df["zscores"] > 3].index
working_df.drop(length_remove, inplace = True)
working_df = working_df.drop("zscores", axis = 1)

plt.hist(working_df["length_title"])
plt.title("Title Length Histogram")
plt.xlabel("Title Length")
plt.ylabel("Count")
plt.show() # There is still some right skew but otherwise this looks like a reasonable distriubtion.

print(working_df.describe()) # The final dataset has 885,527 rows and 6 columns.